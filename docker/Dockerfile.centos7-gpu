FROM nvidia/cuda:10.0-devel-centos7 as builder

RUN yum install -y \
        boost-static \
        gcc \
        gcc-c++ \
        make \
        python \
        python-devel \
        wget && \
    rm -rf /var/cache/yum/*

WORKDIR /root

RUN wget https://cmake.org/files/v3.12/cmake-3.12.2-Linux-x86_64.tar.gz
RUN tar xf cmake-3.12.2-Linux-x86_64.tar.gz && \
    rm cmake-3.12.2-Linux-x86_64.tar.gz
ENV PATH=$PATH:/root/cmake-3.12.2-Linux-x86_64/bin

ENV MKL_VERSION=2019
ENV MKL_UPDATE=4
ENV MKL_BUILD=070
RUN yum-config-manager --add-repo https://yum.repos.intel.com/mkl/setup/intel-mkl.repo && \
    rpm --import https://yum.repos.intel.com/intel-gpg-keys/GPG-PUB-KEY-INTEL-SW-PRODUCTS-2019.PUB && \
    yum install -y intel-mkl-64bit-$MKL_VERSION.$MKL_UPDATE-$MKL_BUILD && \
    rm -rf /var/cache/yum/*

ENV MKLDNN_ROOT=/root/mkl-dnn
ENV MKLDNN_VERSION=0.19
RUN wget https://github.com/intel/mkl-dnn/archive/v$MKLDNN_VERSION.tar.gz && \
    tar xf v$MKLDNN_VERSION.tar.gz && rm v$MKLDNN_VERSION.tar.gz && \
    cd mkl-dnn-* && \
    cd scripts && ./prepare_mkl.sh && cd .. && \
    mkdir build && cd build && \
    cmake -DCMAKE_INSTALL_PREFIX=${MKLDNN_ROOT} \
          -DARCH_OPT_FLAGS="" -DMKLDNN_USE_MKL=ML -DMKLDNN_THREADING=OMP:INTEL \
          -DWITH_TEST=OFF -DWITH_EXAMPLE=OFF .. && \
    make -j4 && make install && \
    cd ../.. && rm -r mkl-dnn-*

ENV CUDNN_VERSION_SHORT=7.6.1
ENV CUDNN_VERSION=${CUDNN_VERSION_SHORT}.34
RUN curl -fsSL http://developer.download.nvidia.com/compute/redist/cudnn/v${CUDNN_VERSION_SHORT}/cudnn-10.0-linux-x64-v${CUDNN_VERSION}.tgz -O && \
    tar --no-same-owner -xzf cudnn-10.0-linux-x64-v${CUDNN_VERSION}.tgz -C /usr/local && \
    rm cudnn-10.0-linux-x64-v${CUDNN_VERSION}.tgz

ENV TENSORRT_ROOT=/root/TensorRT
COPY deps/TensorRT-* /root/
RUN tar -xzf TensorRT-* && \
    rm TensorRT-*.tar.gz && \
    mv TensorRT-* ${TENSORRT_ROOT} && \
    rm ${TENSORRT_ROOT}/lib/*.a && \
    rm ${TENSORRT_ROOT}/lib/libnvcaffe_parser* && \
    rm ${TENSORRT_ROOT}/lib/libnvinfer_plugin* && \
    rm ${TENSORRT_ROOT}/lib/libnvonnxparser* && \
    rm ${TENSORRT_ROOT}/lib/libnvparsers*

ENV CUB_VERSION=1.8.0
ENV CUB_ROOT=/root/cub
RUN wget https://github.com/NVlabs/cub/archive/v${CUB_VERSION}.tar.gz && \
    tar xf v${CUB_VERSION}.tar.gz && \
    mv cub-${CUB_VERSION} ${CUB_ROOT} && \
    rm v${CUB_VERSION}.tar.gz

WORKDIR /root/ctranslate2-dev

COPY mkl_symbol_list .
COPY cli cli
COPY include include
COPY src src
COPY tests tests
COPY CMakeLists.txt .

ARG CXX_FLAGS
ENV CXX_FLAGS=${CXX_FLAGS}
ARG CUDA_NVCC_FLAGS
ENV CUDA_NVCC_FLAGS=${CUDA_NVCC_FLAGS:-"-Xfatbin -compress-all"}
ARG CUDA_ARCH_LIST
ENV CUDA_ARCH_LIST=${CUDA_ARCH_LIST:-"Common"}

RUN mkdir build && \
    cd build && \
    cmake -DCMAKE_INSTALL_PREFIX=/root/ctranslate2 \
          -DCMAKE_PREFIX_PATH="${CUB_ROOT};${MKLDNN_ROOT};${TENSORRT_ROOT}" \
          -DWITH_CUDA=ON -DWITH_MKLDNN=ON \
          -DCMAKE_BUILD_TYPE=Release -DCMAKE_CXX_FLAGS="${CXX_FLAGS}" \
          -DCUDA_NVCC_FLAGS="${CUDA_NVCC_FLAGS}" -DCUDA_ARCH_LIST="${CUDA_ARCH_LIST}" .. && \
    VERBOSE=1 make -j4 && \
    make install

COPY python python

WORKDIR /root/ctranslate2-dev/python
RUN curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py && python get-pip.py && rm get-pip.py
RUN pip --no-cache-dir install setuptools wheel
RUN CFLAGS="-DWITH_CUDA=ON -DWITH_MKL=ON" CTRANSLATE2_ROOT=/root/ctranslate2 \
    python setup.py bdist_wheel

WORKDIR /root
RUN cp /opt/intel/lib/intel64/libiomp5.so /root/ctranslate2/lib && \
    cp -P /root/mkl-dnn/lib64/libmkldnn.so* /root/ctranslate2/lib && \
    cp -P /usr/lib64/libboost_python*.so* /root/ctranslate2/lib && \
    cp -P /usr/local/cuda/lib*/libcudnn.so* /root/ctranslate2/lib && \
    cp -P /root/TensorRT/lib*/*.so* /root/ctranslate2/lib && \
    cp /root/ctranslate2-dev/python/dist/*whl /root/ctranslate2

FROM nvidia/cuda:10.0-base-centos7

RUN yum install -y \
        cuda-cublas-$CUDA_PKG_VERSION && \
    rm -rf /var/cache/yum/*

COPY --from=builder /root/ctranslate2 /opt/ctranslate2
RUN curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py && python get-pip.py && rm get-pip.py
RUN pip --no-cache-dir install /opt/ctranslate2/*.whl

WORKDIR /opt

ENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/ctranslate2/lib

ENTRYPOINT ["/opt/ctranslate2/bin/translate"]
