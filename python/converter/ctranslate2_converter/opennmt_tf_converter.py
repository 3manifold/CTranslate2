import argparse
import shutil
import os
import six
import numpy as np
import tensorflow as tf

from ctranslate2_converter.converter import Converter
from ctranslate2_converter.specs import transformer_spec


def load_model(model_dir, src_vocab=None, tgt_vocab=None):
    """Loads variables and vocabularies from a TensorFlow checkpoint or SavedModel."""
    if tf.saved_model.maybe_saved_model_directory(model_dir):
        with tf.Session() as sess:
            meta_graph = tf.saved_model.loader.load(sess, ["serve"], model_dir)
            variables = sess.run(
                {variable.op.name:variable for variable in tf.trainable_variables()})
            assets = sess.run(tf.get_collection(tf.GraphKeys.ASSET_FILEPATHS))
        src_vocab = os.path.join(model_dir, "assets", os.path.basename(assets[0]))
        tgt_vocab = os.path.join(model_dir, "assets", os.path.basename(assets[1]))
    else:
        if src_vocab is None or tgt_vocab is None:
            raise ValueError("vocabularies must be passed as argument when converting checkpoint")
        reader = tf.train.load_checkpoint(model_dir)
        variables = {
            name:reader.get_tensor(name)
            for name in six.iterkeys(reader.get_variable_to_shape_map())}
    return variables, src_vocab, tgt_vocab


class OpenNMTTFConverter(Converter):
    """Converts models generated by OpenNMT-tf."""

    def __init__(self, model_dir, src_vocab=None, tgt_vocab=None):
        self._model_dir = model_dir
        self._src_vocab = src_vocab
        self._tgt_vocab = tgt_vocab

    def _load(self, model_type):
        variables, src_vocab, tgt_vocab = load_model(
            self._model_dir,
            src_vocab=self._src_vocab,
            tgt_vocab=self._tgt_vocab)
        if model_type == "transformer":
            spec = make_transformer_spec(variables)
        else:
            raise ValueError("Unsupported model type %s" % model_type)
        return spec, src_vocab, tgt_vocab

    def _save_vocabulary(self, vocab, destination):
        shutil.copy(vocab, destination)


def make_transformer_spec(variables):
    # TODO: detect number of layers.
    spec = transformer_spec.TransformerSpec(6, fused_projections=True)
    set_transformer_encoder(spec.encoder, variables)
    set_transformer_decoder(spec.decoder, variables)
    return spec

def set_transformer_encoder(spec, variables):
    set_layer_norm(spec.layer_norm, variables, "transformer/encoder/LayerNorm")
    set_embeddings(spec.embeddings, variables, "transformer/encoder")
    for i, layer in enumerate(spec.layer):
        set_transformer_encoder_layer(layer, variables, "transformer/encoder/layer_%d" % i)

def set_transformer_decoder(spec, variables):
    set_linear(spec.projection, variables, "transformer/decoder/dense")
    set_layer_norm(spec.layer_norm, variables, "transformer/decoder/LayerNorm")
    set_embeddings(spec.embeddings, variables, "transformer/decoder")
    for i, layer in enumerate(spec.layer):
        set_transformer_decoder_layer(layer, variables, "transformer/decoder/layer_%d" % i)

def set_transformer_encoder_layer(spec, variables, scope):
    set_ffn(spec.ffn, variables, "%s/ffn" % scope)
    set_multi_head_attention(
        spec.self_attention, variables, "%s/multi_head" % scope, self_attention=True)

def set_transformer_decoder_layer(spec, variables, scope):
    set_ffn(spec.ffn, variables, "%s/ffn" % scope)
    set_multi_head_attention(
        spec.self_attention, variables, "%s/masked_multi_head" % scope, self_attention=True)
    set_multi_head_attention(spec.attention, variables, "%s/multi_head" % scope)

def set_ffn(spec, variables, scope):
    set_layer_norm(spec.layer_norm, variables, "%s/LayerNorm" % scope)
    set_linear(spec.linear_1, variables, "%s/conv1d" % scope)
    set_linear(spec.linear_2, variables, "%s/conv1d_1" % scope)

def set_multi_head_attention(spec, variables, scope, self_attention=False):
    set_layer_norm(spec.layer_norm, variables, "%s/LayerNorm" % scope)
    set_linear(spec.linear[0], variables, "%s/conv1d" % scope)
    set_linear(spec.linear[1], variables, "%s/conv1d_1" % scope)
    if not self_attention:
        set_linear(spec.linear[2], variables, "%s/conv1d_2" % scope)

def set_layer_norm(spec, variables, scope):
    spec.gamma = variables["%s/gamma" % scope]
    spec.beta = variables["%s/beta" % scope]

def set_linear(spec, variables, scope):
    spec.weight = np.transpose(np.squeeze(variables["%s/kernel" % scope]))
    spec.bias = variables["%s/bias" % scope]

def set_embeddings(spec, variables, scope):
    spec.weight = variables["%s/w_embs" % scope]


if __name__ == '__main__':
    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument("--model_dir", required=True,
                        help="Model directory.")
    parser.add_argument("--src_vocab", default=None,
                        help="Source vocabulary file (required if converting a checkpoint).")
    parser.add_argument("--tgt_vocab", default=None,
                        help="Target vocabulary file (required if converting a checkpoint).")
    Converter.declare_arguments(parser)
    args = parser.parse_args()
    OpenNMTTFConverter(
        args.model_dir,
        src_vocab=args.src_vocab,
        tgt_vocab=args.tgt_vocab).convert_from_args(args)
